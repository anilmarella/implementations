{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering\n",
    "\n",
    "This notebook has implementation of Collaborative filtering by optimizing a single cost function to learn both User features and Product features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing User featuers and Product features.\n",
    "\n",
    "$n_{u}$ is the number of unique users.  \n",
    "$n_{m}$ is the number of unique products.  \n",
    "$n_{f}$ is the size of the feature vector that we want to use for both the user and the product.  \n",
    "    \n",
    "We use normal distribution for both the user and product features with mean 0 and standard deviation 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_features(n_u, n_m, n_f):\n",
    "    # mean and standard deviation\n",
    "    mu, sigma = 0, 0.1 \n",
    "    user_features = np.random.normal(mu, sigma, (n_u, n_f))\n",
    "    prod_features = np.random.normal(mu, sigma, (n_m, n_f))\n",
    "    return user_features, prod_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Ratings for given users and products\n",
    "\n",
    "Since, taking a dot product of user_features and product_features will result in a $n_{u} x n_{m}$ matrix which runs out of memory. We use [scipy sparse matrix implementation](https://docs.scipy.org/doc/scipy/reference/sparse.html) library to avoid memory error.\n",
    "\n",
    "The inputs to predict_ratings method are $user\\_features, product\\_features$ and $users, products$. The $users, products$ are list of users who rated respective products. This can be used to index user_features and product_features to ensure that the result matrix is sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ratings(user_features, prod_features, users, products):\n",
    "    predicted_ratings = np.sum(np.multiply(user_features[users,:], prod_features[products,:]), axis=1)\n",
    "    return csr_matrix((predicted_ratings, (users, products)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing cost \n",
    "\n",
    "Given the features and predictions of ratings based on the features.\n",
    "\n",
    "Cost function <b>J</b> is given by  \n",
    "$$ J (x^{(1)},...,x^{(n_{m})}, c^{(1)},...,c^{(n_{u})}) = \\frac{1}{2}\\sum_{(i,j):r(i,j)=1} ((c^{(j)})^{T}x^{(i)} - y^{(i,j)})  +  \\frac{\\lambda}{2}\\sum_{i=1}^{n_{m}}\\sum_{k=1}^{n}(x_{k}^{(i)})^{2}  +  \\frac{\\lambda}{2}\\sum_{i=1}^{n_{m}}\\sum_{k=1}^{n}(c_{k}^{(i)})^{2} $$\n",
    "\n",
    "Where,  \n",
    "$ x^{(i)} $ is the $i^{th}$ product's feature vector. It is of dimension (n_f, 1)  \n",
    "$ c^{(i)} $ is the $i^{th}$ user's feature vector. It is of dimension (n_f, 1)  \n",
    "$ x_{k}^{(i)} $ is the $k^{th}$ feature of $i^{th}$ product's feature vector.  \n",
    "$ c_{k}^{(i)} $ is the $k^{th}$ feature of $i^{th}$ user's feature vector.  \n",
    "$ (c^{(j)})^{T}x^{(i)} $ is the predicted rating given by $j^{th}$ user to $i^{th}$ product  \n",
    "$ y^{(i,j)} $ is the actual rating given by $j^{th}$ user to $i^{th}$ product  \n",
    "$ \\lambda $ is the regularization factor.  \n",
    "$ r^{(i,j)} = 1$ if $i^{th}$ product is rated by $j^{th}$ user otherwise it is $0$. This is not used in our computation as it is taken care in the predict_ratings method by using indexing based on $user, products$ columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(user_features, prod_features, prediction_sparse_matrix, ratings_sparse_matrix, lambd=0.1):\n",
    "    user_regularization = (lambd/2)*np.sum(np.sum(np.square(user_features)))\n",
    "    song_regularization = (lambd/2)*np.sum(np.sum(np.square(song_features)))\n",
    "    cost = (1/2)*(prediction_sparse_matrix - ratings_sparse_matrix).power(2).sum() + user_regularization + song_regularization\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing gradients\n",
    "\n",
    "To learn the parameters, we have to minimize the cost function $ J (x^{(1)},...,x^{(n_{m})}, c^{(1)},...,c^{(n_{u})}) $ given above with respect to both $ x^{(i)} $ and $ c^{(i)} $\n",
    "\n",
    "The derivative for the cost function with respect to $ x^{(i)} $ is\n",
    "$$\\frac{\\mathrm{d}J}{\\mathrm{d}x_{k}^{(i)}}  =  \\sum_{j:r(i,j)=1} ((c^{(j)})^{T}x^{(i)} - y^{(i,j)})c_{k}^{(j)}  +  \\lambda x_{k}^{(i)} $$\n",
    "\n",
    "The derivative for the cost function with respect to $ c^{(i)} $ is\n",
    "$$\\frac{\\mathrm{d}J}{\\mathrm{d}c_{k}^{(i)}}  =   \\sum_{i:r(i,j)=1} ((c^{(j)})^{T}x^{(i)} - y^{(i,j)})x_{k}^{(i)}  +  \\lambda c_{k}^{(j)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(user_features, song_features, predicted_ratings_sparse, actual_ratings_sparse, lambd=0.1):\n",
    "    prediction_offset_sparse = predicted_ratings_sparse - actual_ratings_sparse\n",
    "    d_users = csr_matrix.dot(prediction_offset_sparse, song_features) + lambd*user_features\n",
    "    d_songs = csr_matrix.dot(csr_matrix.transpose(prediction_offset_sparse), user_features) + lambd*song_features\n",
    "    return d_users, d_songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating parameters by using computed gradients\n",
    "\n",
    "$$ x_{k}^{(i)}  =  x_{k}^{(i)}  -  \\alpha\\frac{\\mathrm{d}J}{\\mathrm{d}x_{k}^{(i)}} $$\n",
    "$$ c_{k}^{(i)}  =  c_{k}^{(i)}  -  \\alpha\\frac{\\mathrm{d}J}{\\mathrm{d}c_{k}^{(i)}} $$\n",
    "\n",
    "Where, $\\alpha$ is the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_features(user_features, song_features, d_users, d_songs, learning_rate=0.01):\n",
    "    user_features = user_features - learning_rate*d_users\n",
    "    song_features = song_features - learning_rate*d_songs\n",
    "    return user_features, song_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comupte error\n",
    "\n",
    "This is similar to the first part of the cost function. It is the Square root of sum of squared errors for each prediction. This is a useful evaluation metric\n",
    "\n",
    "$$ \\sqrt{\\sum_{(i,j):r(i,j)=1} ((c^{(j)})^{T}x^{(i)} - y^{(i,j)})^{2}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(true_ratings, predicted_ratings):\n",
    "    return np.sqrt((predicted_ratings - true_ratings).power(2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the dataset\n",
    "\n",
    "For this algorithm, we will use the [songs dataset from kaggle](https://www.kaggle.com/rymnikski/dataset-for-collaborative-filters)  \n",
    "To split the train and test data, we randomly select one observation for each user as a test observation and use the square root of sum of squared errors to determine the goodness of the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "data = pd.read_csv('data/songsDataset.csv')\n",
    "data['songIDx'] = data['songID'].astype('category')\n",
    "data['songIDx'] = data['songIDx'].cat.codes\n",
    "data['pkey'] = np.arange(data.shape[0])\n",
    "\n",
    "# Selecting one rating by each user to test properly\n",
    "test_data = data.groupby(['userID'], as_index=False).apply(lambda x : x.loc[np.random.choice(x.index, replace=False)])\n",
    "train_data = data.loc[~data['pkey'].isin(test_data['pkey']), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declaring some constants and verifying the dataset.\n",
    "\n",
    "we will use,  \n",
    "$n\\_f$ to denote number of features.  \n",
    "$n\\_u$ to denote number of users.  \n",
    "$n\\_m$ to denote number of songs.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_f = 5\n",
    "n_u = data['userID'].unique().shape[0]\n",
    "n_m = data['songID'].unique().shape[0]\n",
    "num_epochs = 51\n",
    "lambd = 1\n",
    "learning_rate = 0.01\n",
    "\n",
    "print('Total number of observations in training data is {}'.format(train_data.shape[0]))\n",
    "print('Total number of observations in testing data is {}'.format(test_data.shape[0]))\n",
    "print('Number of features for both products and users is {}'.format(n_f))\n",
    "print('Number of unique users in the data is {}'.format(n_u))\n",
    "print('Number of unique products in the data is {}'.format(n_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the ratings sparse matrix\n",
    "\n",
    "We use scipy sparse matrix library to construct actual ratings sparse matrix for both the training and the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the train_ratings sparse matrix\n",
    "train_users = train_data['userID']\n",
    "train_songs = train_data['songIDx']\n",
    "train_ratings = csr_matrix((train_data['rating'], (train_users, train_songs)))\n",
    "\n",
    "# Constructing the test_ratings sparse matrix\n",
    "test_users = test_data['userID']\n",
    "test_songs = test_data['songIDx']\n",
    "test_ratings = csr_matrix((test_data['rating'], (test_users, test_songs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "user_features, song_features = initialize_features(n_u, n_m, n_f)\n",
    "for eph in range(num_epochs):\n",
    "    predicted_ratings_sparse = predict_ratings(user_features, song_features, train_users, train_songs)\n",
    "    print(predicted_ratings_sparse[0,6706])\n",
    "    if(eph % 2 == 0):\n",
    "        print(\"Cost at epoch {} is {}\".format(eph, compute_cost(user_features, song_features, predicted_ratings_sparse, train_ratings, lambd)))\n",
    "    if(eph % 5 == 0):\n",
    "        test_predicts = predict_ratings(user_features, song_features, test_users, test_songs)\n",
    "        print(\"Mean squared error on test data after epoch {} is {}\".format(eph, compute_error(test_ratings, test_predicts)))\n",
    "        print(\"Mean squared error on train data after epoch {} is {}\".format(eph, compute_error(train_ratings, predicted_ratings_sparse)))\n",
    "    d_users, d_songs = compute_gradients(user_features, song_features, predicted_ratings_sparse, train_ratings, lambd)\n",
    "    user_features, song_features = update_features(user_features, song_features, d_users, d_songs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
