{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural network implementation\n",
    "\n",
    "This notebook has end to end deep neural network implementation from scratch.\n",
    "\n",
    "1. This implementation uses only sigmoid activation\n",
    "2. Optimization techniques implemented are classic gradient descent, gradient descent with momentum, RMSProp and  Adam optimization. All of these are mini-batched by default\n",
    "3. The deep neural network is implemented only with L2 - Regularization\n",
    "\n",
    "**Note:** The notebook has many similarities to the programming assignments from [Coursera Deep learning courses](https://www.coursera.org/specializations/deep-learning \"Deep learning Specialization\") because that is where I learned it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sigmoid function:** The expected input is a scalar or a vector or a tensor of any shape.\n",
    "\n",
    "Returns sigmoid of each of the element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    act = 1/(1+np.exp(-z))\n",
    "    \n",
    "    return act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sigmoid derivative function:** The expected input is a scalar or a vector or a tensor of any shape. \n",
    "\n",
    "Returns derivative of sigmoid that is g'(a) at the given point 'a' for each of the element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_derivative(a):\n",
    "    return np.multiply(a, (1-a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize weights:** This method is called for each layer. It expects two arguments that is number of input nodes and number of output nodes. The weights are initialized using Xavier initialization. We are using only sigmoid activations. The biases are intialized to zero\n",
    "\n",
    "Returns both Weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(n_input_nodes, n_output_nodes):\n",
    "    W = np.random.randn(n_output_nodes, n_input_nodes)*np.sqrt(1/n_input_nodes)\n",
    "    b = np.zeros(shape=[n_output_nodes, 1])\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating layers:** This method will create hidden layers by initializing weights with appropriate dimensions.\n",
    "The expected argument is a list of layer sizes. The list will contain both the number of units in input layer and number of units in the output layer. These two are actually appended to the hidden layers list in the train_nn_model method.\n",
    "\n",
    "Returns a dictionary of parameters with keys as W_i_ and b_i_ corresponding to weights and biases of i<sup>th</sup> layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_layers(layers_list):\n",
    "    parameters = {}\n",
    "    for idx in range(1, len(layers_list)):\n",
    "        parameters['W'+str(idx)], parameters['b'+str(idx)] = initialize_parameters(layers_list[idx-1], layers_list[idx])\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward propagation:** The expected arguments X and parameters have to follow the shape requirements.\n",
    "1. X' is expected to have [n_features, n_observations]\n",
    "2. parameters have two types W, b\n",
    "    - 'Wi' are expected to have shape [n<sup>[i]</sup>, n<sup>[i-1]</sup>]\n",
    "    - 'bi' are expected to have shape [n<sup>[i]</sup>, 1]\n",
    "\n",
    "All the returned activations will have shape [n<sup>[i]</sup>, 1]. Where, n<sup>[i]</sup> indicates number of units in the i<sup>th</sup> layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward porpagation equations:**  First/Input layer is considered to be 0<sup>th</sup> or, mathematically A<sup>[0]</sup> = X\n",
    "<center>Z<sup>[l+1]</sup> = W<sup>[l+1]</sup> x A<sup>[l]</sup> + b<sup>[l+1]</sup></center>\n",
    "<center>A<sup>[l+1]</sup> = sigmoid(Z<sup>[l+1]</sup>)</center>\n",
    "Where, l ranges from **0 to L** where, L is index of output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    activations = {}\n",
    "    activations['A0'] = X\n",
    "    limit = 1 + len(parameters)//2\n",
    "    for idx in range(1, limit):\n",
    "        Z = np.dot(parameters['W'+str(idx)], activations['A'+str(idx-1)]) + parameters['b'+str(idx)]\n",
    "        activations['A'+str(idx)] = sigmoid(Z)\n",
    "\n",
    "    return activations, activations['A'+str(limit-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute cost:** The expected arguments are the computed or predicted labels and actual labels. The requirements is both the inputs are supposed to have shape [1, n_observation]\n",
    "\n",
    "Returns average log loss for the given input vector of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(act, y, parameters, lambd, regularization='l2'):\n",
    "    m = y.shape[1]\n",
    "    logprobs = np.multiply(np.log(act), y) + np.multiply(np.log(1-act), 1-y)\n",
    "    \n",
    "    if regularization == 'l2':\n",
    "        reg_term = 0\n",
    "        for i in range(1, 1+ len(parameters)//2):\n",
    "            reg_term += np.sum(np.square(parameters['W'+str(i)]))\n",
    "        reg_term = (lambd/(2*m))*reg_term\n",
    "    else:\n",
    "        reg_term = 0\n",
    "    \n",
    "    return (-1/m)*np.sum(logprobs) + reg_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward Propagation:** Perform backward propagation with the given parameters and activations after forward propagation. The expected arguments are actual labels y, parameters (Wi and bi) and activations (Ai) after forward propagation. Of course all the dimensions should be appropriate.\n",
    "\n",
    "Returns a dictionary of gradients with keys dW_i_ and db_i_ for each layer 'i' which can be used to perform gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_propagation(y, parameters, activations, lambd, regularization='l2'):\n",
    "    m = y.shape[0]\n",
    "    last = len(parameters)//2\n",
    "    gradients = {}\n",
    "    dz_dict = {}\n",
    "    dz_dict['dZ'+str(last)] = activations['A'+str(last)] - y\n",
    "    for l in range(last-1, 0, -1):\n",
    "        dz_dict['dZ'+str(l)] = np.multiply(sigmoid_derivative(activations['A'+str(l)]), np.dot(parameters['W'+str(l+1)].T, dz_dict['dZ'+str(l+1)]))\n",
    "    for i in range(last, 0, -1):\n",
    "        gradients['dW'+str(i)] = (1/m) * np.dot(dz_dict['dZ'+str(i)], activations['A'+str(i-1)].T)\n",
    "        gradients['db'+str(i)] = (1/m) * np.sum(dz_dict['dZ'+str(i)], axis=1, keepdims=True)\n",
    "\n",
    "    if regularization == 'l2':\n",
    "        for i in range(1, 1+ len(parameters)//2):\n",
    "            gradients['dW'+str(i)] = gradients['dW'+str(i)] + (lambd/m)*parameters['W'+str(i)]\n",
    "            \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update parameters:** will update the parameters based on their existing value and gradients given a learning rate. The expected arguments are parameters, gradients and learning_rate.\n",
    "\n",
    "Returns dictionary of updated parameters in the same format as the input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classical gradient descent batch/mini-batch\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    lim = 1 + len(parameters)//2\n",
    "    for i in range(1, lim):\n",
    "        parameters['W'+str(i)] -= learning_rate*gradients['dW'+str(i)]\n",
    "        parameters['b'+str(i)] -= learning_rate*gradients['db'+str(i)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialization for Gradient descent with momentum\n",
    "def initialize_momentum_params(parameters):\n",
    "    lim = 1 + len(parameters)//2\n",
    "    v = {}\n",
    "    for i in range(1, lim):\n",
    "        v['dW'+str(i)] = np.zeros(parameters['W'+str(i)].shape)\n",
    "        v['db'+str(i)] = np.zeros(parameters['b'+str(i)].shape)\n",
    "        \n",
    "    return v\n",
    "\n",
    "# Gradient descent with momentum\n",
    "def update_parameters_momentum(parameters, gradients, learning_rate, v, beta=0.9):\n",
    "    lim = 1 + len(parameters)//2\n",
    "    for i in range(1, lim):\n",
    "        v['dW'+str(i)] = beta*v['dW'+str(i)] + (1-beta)*gradients['dW'+str(i)]\n",
    "        v['db'+str(i)] = beta*v['db'+str(i)] + (1-beta)*gradients['db'+str(i)]\n",
    "        parameters['W'+str(i)] -= learning_rate*v['dW'+str(i)]\n",
    "        parameters['b'+str(i)] -= learning_rate*v['db'+str(i)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialization for Gradient descent with RMSProp\n",
    "def initialize_rmsprop_params(parameters):\n",
    "    lim = 1 + len(parameters)//2\n",
    "    s={}\n",
    "    for i in range(1, lim):\n",
    "        s['dW'+str(i)] = np.zeros(parameters['W'+str(i)].shape)\n",
    "        s['db'+str(i)] = np.zeros(parameters['b'+str(i)].shape)\n",
    "        \n",
    "    return s\n",
    "\n",
    "# Gradient descent with RMSProp implementation\n",
    "def update_parameters_rmsprop(parameters, gradients, learning_rate, s, t, beta=0.999, epsilon=1e-8):\n",
    "    s_corrected={}\n",
    "    lim = 1 + len(parameters)//2\n",
    "    for i in range(1, lim):\n",
    "        s['dW'+str(i)] = beta*s['dW'+str(i)] + (1-beta)*gradients['dW'+str(i)]**2\n",
    "        s['db'+str(i)] = beta*s['db'+str(i)] + (1-beta)*gradients['db'+str(i)]**2\n",
    "        \n",
    "        s_corrected[\"dW\" + str(i)] = s[\"dW\"+str(i)]/(1-beta**t)\n",
    "        s_corrected[\"db\" + str(i)] = s[\"db\"+str(i)]/(1-beta**t)\n",
    "        \n",
    "        parameters['W'+str(i)] -= learning_rate*1/(epsilon + np.sqrt(s_corrected['dW'+str(i)]))\n",
    "        parameters['b'+str(i)] -= learning_rate*1/(epsilon + np.sqrt(s_corrected['db'+str(i)]))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialization for Adam optimization\n",
    "def initialize_adam_params(parameters):\n",
    "    lim = 1 + len(parameters)//2\n",
    "    v = {};s={}\n",
    "    for i in range(1, lim):\n",
    "        v['dW'+str(i)] = np.zeros(parameters['W'+str(i)].shape)\n",
    "        s['dW'+str(i)] = np.zeros(parameters['W'+str(i)].shape)\n",
    "        v['db'+str(i)] = np.zeros(parameters['b'+str(i)].shape)\n",
    "        s['db'+str(i)] = np.zeros(parameters['b'+str(i)].shape)\n",
    "    return v, s\n",
    "\n",
    "# Adam optimization implementation\n",
    "def update_parameters_adam(parameters, gradients, learning_rate, v, s, t, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    v_corrected={};s_corrected={}\n",
    "    lim = 1 + len(parameters)//2\n",
    "    for i in range(1, lim):\n",
    "        v['dW'+str(i)] = beta1*v['dW'+str(i)] + (1-beta1)*gradients['dW'+str(i)]\n",
    "        v['db'+str(i)] = beta1*v['db'+str(i)] + (1-beta1)*gradients['db'+str(i)]\n",
    "\n",
    "        s['dW'+str(i)] = beta2*s['dW'+str(i)] + (1-beta2)*gradients['dW'+str(i)]**2\n",
    "        s['db'+str(i)] = beta2*s['db'+str(i)] + (1-beta2)*gradients['db'+str(i)]**2\n",
    "        \n",
    "        v_corrected[\"dW\" + str(i)] = v[\"dW\"+str(i)]/(1-beta1**t)\n",
    "        v_corrected[\"db\" + str(i)] = v[\"db\"+str(i)]/(1-beta1**t)\n",
    "        \n",
    "        s_corrected[\"dW\" + str(i)] = s[\"dW\"+str(i)]/(1-beta2**t)\n",
    "        s_corrected[\"db\" + str(i)] = s[\"db\"+str(i)]/(1-beta2**t)\n",
    "        \n",
    "        parameters['W'+str(i)] -= learning_rate*(v_corrected[\"dW\"+str(i)]/(epsilon + np.sqrt(s_corrected[\"dW\"+str(i)])))\n",
    "        parameters['b'+str(i)] -= learning_rate*(v_corrected[\"db\"+str(i)]/(epsilon + np.sqrt(s_corrected[\"db\"+str(i)])))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict output:** This method will return the prediction for a given data. The data can be any number of observations that match [n_features, n_observations]\n",
    "\n",
    "Returns a vector of shape [1, n_observations] containing 0 if the predicted probability is less than or equal 0.5 and 1 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    _, op = forward_propagation(X, parameters)\n",
    "    predictions = np.zeros(op.shape)\n",
    "    predictions[op > 0.5] = 1;\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate mini batches:** This method randomizes both input features and labels and returns batches of given size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1,m))\n",
    "\n",
    "    num_complete_minibatches = 1 + math.floor(m/mini_batch_size)\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Neural network:** This method expects at least X, y and hidden_layers. If no hidden layer is give, a logistic regression is performed. X has to have shape [n_features, n_observations] and y has to have shape [1, n_observation]\n",
    "hidden_layers is a list of number of neurons excluding input and output layer.\n",
    "\n",
    "Returns parameters or weights of a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_nn_model(X, y, hidden_layers=[], epochs=1500, learning_rate=0.0001, lambd=2.0, regularization='l2', optimization='adam', beta1=0.9, beta2=0.999, epsilon=1e-8, t=2):\n",
    "\n",
    "    layers = [X.shape[0]]\n",
    "    layers.extend(hidden_layers)\n",
    "    layers.append(1)\n",
    "    \n",
    "    # Initializing parameters\n",
    "    parameters = create_layers(layers_list=layers)\n",
    "    \n",
    "    if optimization == 'rmsprop':\n",
    "        s = initialize_rmsprop_params(parameters)\n",
    "    elif optimization == 'adam':\n",
    "        v, s = initialize_adam_params(parameters)\n",
    "    elif optimization == 'momentum':\n",
    "        v = initialize_momentum_params(parameters)\n",
    "    \n",
    "    mini_batch_size = 64\n",
    "    for epoch in range(1, 1+epochs):\n",
    "        \n",
    "        cost = 0\n",
    "        minibatches = generate_random_mini_batches(X, y, mini_batch_size)\n",
    "\n",
    "        for minibatch in minibatches:\n",
    "\n",
    "            # Select a minibatch\n",
    "            (minibatch_X, minibatch_y) = minibatch\n",
    "        \n",
    "            # forward propagation\n",
    "            activations, prediction = forward_propagation(minibatch_X, parameters)\n",
    "\n",
    "            # backward propagation\n",
    "            grads = backward_propagation(minibatch_y, parameters, activations, lambd, regularization)\n",
    "\n",
    "            # calculate cost\n",
    "            cost += compute_cost(prediction, minibatch_y, parameters, lambd, regularization)\n",
    "            \n",
    "            #updating parameters\n",
    "            if optimization == 'rmsprop':\n",
    "                parameters = update_parameters_rmsprop(parameters, grads, learning_rate, s, t, beta2, epsilon)\n",
    "            elif optimization == 'adam':\n",
    "                parameters = update_parameters_adam(parameters, grads, learning_rate, v, s, t, beta1, beta2, epsilon)\n",
    "            elif optimization == 'momentum':\n",
    "                parameters = update_parameters_momentum(parameters, grads, learning_rate, v, beta1)\n",
    "            else:\n",
    "                parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print (\"Average training cost after Epoch {}: {}\".format(epoch, cost/len(minibatches)))        \n",
    "            \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy:** This method expects input features, labels and trained parameters of appropriate shape.\n",
    "    \n",
    "Returns average prediction rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(X, y, parameters):\n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m))\n",
    "    _, act = forward_propagation(X, parameters)\n",
    "    for i in range(0, act.shape[1]):\n",
    "        if act[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "   \n",
    "    return np.mean((p[0,:] == y[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training cost after Epoch 100: 0.7159551448487017\n",
      "Average training cost after Epoch 200: 0.6587144138445543\n",
      "Average training cost after Epoch 300: 0.626463145638603\n",
      "Average training cost after Epoch 400: 0.6303553397159969\n",
      "Average training cost after Epoch 500: 0.602919106147456\n",
      "Average training cost after Epoch 600: 0.5932983166184365\n",
      "Average training cost after Epoch 700: 0.6059659811007394\n",
      "Average training cost after Epoch 800: 0.5733825507302504\n",
      "Average training cost after Epoch 900: 0.5791760241969467\n",
      "Average training cost after Epoch 1000: 0.5804971843980433\n",
      "Average training cost after Epoch 1100: 0.5681642326384855\n",
      "Average training cost after Epoch 1200: 0.5701090523760469\n",
      "Average training cost after Epoch 1300: 0.5722801088266866\n",
      "Average training cost after Epoch 1400: 0.5594267332026424\n",
      "Average training cost after Epoch 1500: 0.5671255078828046\n",
      "Accuracy on the training set: 0.933014354067\n",
      "Accuracy on the test set: 0.64\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "def test():\n",
    "    train_dataset = h5py.File('data/train_catvnoncat.h5', \"r\")\n",
    "    train_x = np.array(train_dataset[\"train_set_x\"][:])\n",
    "    train_x = train_x.reshape(train_x.shape[0], -1).T\n",
    "    train_y = np.array(train_dataset[\"train_set_y\"][:])\n",
    "    train_y = train_y.reshape(1, train_y.shape[0])\n",
    "    \n",
    "    test_dataset = h5py.File('data/test_catvnoncat.h5', \"r\")\n",
    "    test_x = np.array(test_dataset[\"test_set_x\"][:])\n",
    "    test_x = test_x.reshape(test_x.shape[0], -1).T\n",
    "    test_y = np.array(test_dataset[\"test_set_y\"][:])\n",
    "    test_y = test_y.reshape(1, test_y.shape[0])\n",
    "    \n",
    "    hidden_layers = [10]\n",
    "    train_x = train_x/255\n",
    "    test_x = test_x/255\n",
    "    parameters = train_nn_model(train_x, train_y, hidden_layers=hidden_layers)\n",
    "\n",
    "    print (\"Accuracy on the training set:\", accuracy(train_x, train_y, parameters))\n",
    "    print (\"Accuracy on the test set:\", accuracy(test_x, test_y, parameters))\n",
    "\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
